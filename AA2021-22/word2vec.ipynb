{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and word meaning\n",
    "\n",
    "What is the semantics, the meaning of a word?\n",
    "\n",
    "A common sense hypothesis is to say that the meaning of a word is the real object that the word represents. In this framework, words that are not known to be mapped on real objects (e.g., new words for a reader or just random strings of characters like xvul) have no meaning at all.\n",
    "\n",
    "However, this hypothesis is quite useless in a digital context, where we just have words, not real objects.\n",
    "\n",
    "An alternative approach if the **Distributional Hypothesis** about language and word meaning that states that words that occur in the same contexts tend to have similar meanings. (Harris, 1954) In other words, you shall know a word by the company it keeps . (Firth, 1957)\n",
    "\n",
    "> Harris, Z. S. (1954). Distributional structure. Word, 10(2-3), 146-162.\n",
    "\n",
    "> John R. Firth. A synopsis of linguistic theory 1930–1955. In Studies in Linguistic Analysis, Special volume of the Philological Society, pages 1–32. Firth, John Rupert, Haas William, Halliday, Michael A. K., Oxford, Blackwell Ed., 1957.\n",
    "\n",
    "## Explore the Gensim implementation\n",
    "> Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., & Joulin, A. (2017). Advances in pre-training distributed word representations. arXiv preprint arXiv:1712.09405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(datapath(\"/Users/flint/Data/word2vec/GoogleNews-vectors-negative300.bin\"), \n",
    "                                       binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle 0.7821096181869507\n",
      "cars 0.7423831224441528\n",
      "SUV 0.7160962224006653\n",
      "minivan 0.6907036900520325\n",
      "truck 0.6735789775848389\n",
      "Car 0.6677608489990234\n",
      "Ford_Focus 0.667320191860199\n",
      "Honda_Civic 0.6626849174499512\n",
      "Jeep 0.651133120059967\n",
      "pickup_truck 0.6441438794136047\n"
     ]
    }
   ],
   "source": [
    "for x, y in wv.most_similar('car'):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for word in ['car', 'minivan', 'bicycle', 'airplane']:\n",
    "    vectors.append(wv.get_vector(word))\n",
    "V = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = V.mean(axis=0)\n",
    "v = v - wv.get_vector('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LightHawk', 0.3567371666431427),\n",
       " ('Beaver_floatplane', 0.3410896956920624),\n",
       " ('Bluebills', 0.3352811932563782),\n",
       " ('airplane', 0.32490819692611694),\n",
       " ('Volk_Field', 0.307051420211792),\n",
       " ('Andersland', 0.30294421315193176),\n",
       " ('Expedia_Expedia.com', 0.30243098735809326),\n",
       " ('NASA_Weightless_Wonder', 0.30234652757644653),\n",
       " ('Cessna_###B', 0.2979174852371216),\n",
       " ('propeller_plane', 0.2970975339412689)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy\n",
    "\n",
    "FRANCE : PARIS = ITALY : ?\n",
    "\n",
    "PARIS - FRANCE + ITALY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queen', 0.5515626668930054),\n",
       " ('Oprah_BFF_Gayle', 0.47597548365592957),\n",
       " ('Geoffrey_Rush_Exit', 0.46460166573524475),\n",
       " ('Princess', 0.4533674716949463),\n",
       " ('Yvonne_Stickney', 0.4507041573524475),\n",
       " ('L._Bonauto', 0.4422135353088379),\n",
       " ('gal_pal_Gayle', 0.4408389925956726),\n",
       " ('Alveda_C.', 0.4402790665626526),\n",
       " ('Tupou_V.', 0.4373864233493805),\n",
       " ('K._Letourneau', 0.4351031482219696)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['King', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(\"school professor apple student\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = wv['school']\n",
    "vr = wv['professor']\n",
    "vx = wv['student']\n",
    "m = (vp + vr + vx) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similar_by_vector(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('lecturer', 'school'),\n",
    "    ('lecturer', 'professor'),\n",
    "    ('lecturer', 'student'),\n",
    "    ('lecturer', 'teacher'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity('buy', 'money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = _ # assume there's one document per line, tokens separated by whitespace\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import nltk\n",
    "from string import punctuation\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MO = gensim.models.Word2Vec.load('/Users/flint/Playground/MeaningSpread/w2v-global.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('influenza', 0.7046176195144653),\n",
       " ('h1n1', 0.6910238265991211),\n",
       " ('outbreak', 0.6785882711410522),\n",
       " ('avian', 0.6601735949516296),\n",
       " ('flu', 0.6578388214111328),\n",
       " ('outbreaks', 0.5860258936882019),\n",
       " ('swine', 0.5697133541107178),\n",
       " ('pandemics', 0.5689476728439331),\n",
       " ('epidemic', 0.552070677280426),\n",
       " ('cholera', 0.5491413474082947)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MO.wv.most_similar('pandemic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['twitter']['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(db.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict([(tweet['id'], tweet['text']) for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenize = lambda text: [x.lower() for x in nltk.word_tokenize(text) if x not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [nltk_tokenize(y) for x, y in corpus.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = copy.deepcopy(MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4694460, 6477630)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1.train(data, total_examples=MO.corpus_count, epochs=MO.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h1n1', 0.6190395355224609),\n",
       " ('influenza', 0.5918642282485962),\n",
       " ('outbreak', 0.5802384614944458),\n",
       " ('avian', 0.5463380813598633),\n",
       " ('flu', 0.5393428206443787),\n",
       " ('swine', 0.4824046492576599),\n",
       " ('epidemic', 0.47371941804885864),\n",
       " ('outbreaks', 0.46408894658088684),\n",
       " ('pandemics', 0.46319717168807983),\n",
       " ('h5n1', 0.45786356925964355)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1.wv.most_similar('pandemic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
